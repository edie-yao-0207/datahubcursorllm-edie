package datastreamlake

import (
	"fmt"
	"os"
	"path/filepath"
	"reflect"
	"sort"
	"strings"
	"text/template"

	"github.com/samsarahq/go/oops"

	"samsaradev.io/helpers/fileformatter"
	"samsaradev.io/helpers/ni/filepathhelpers"
	"samsaradev.io/libs/ni/filehelpers"
	"samsaradev.io/team/components"
)

type TeamStreams struct {
	Team    components.TeamInfo
	Streams []DataStream
}

var streamWriterFileTemplate = `// Code generated by taskrunner gen/dataplatform/datastreamwriters; DO NOT EDIT.
package {{removeUnderscores .StreamName}}streamwriters

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/aws/aws-sdk-go/service/firehose"
	"github.com/samsarahq/go/oops"

	{{range (sortImports .Record) -}}
	"{{.}}"
	{{end -}}
)

const (
	bytesInMb = 1e6
	recordSizeLimitBytes        = 1 * bytesInMb  // Firehose Record size limit is 1MB.
	batchSizeLimitBytes         = 4 * bytesInMb // Firehose PutRecord API Batch size limit is 4MB.
	numberOfRecordsInBatchLimit = 500      // Firehose PutRecord API number of records in a batch limit is 500.
)

type {{pascalCase .StreamName}}StreamWriter struct {
	firehoseClient firehoseiface.FirehoseAPI
}

func New{{pascalCase .StreamName}}StreamWriter(firehoseAPI firehoseiface.FirehoseAPI) *{{pascalCase .StreamName}}StreamWriter {
	return &{{pascalCase .StreamName}}StreamWriter{
		firehoseClient: firehoseAPI,
	}
}

func init() {
	fxregistry.MustRegisterDefaultConstructor(New{{pascalCase .StreamName}}StreamWriter)
}

type FirehoseRecords struct {
	records []*firehose.Record
}

// AddFirehoseRecordToBatch adds {{recordType .Record}} to a list of FirehoseRecords after validating the input
func (f *FirehoseRecords) ValidateAndAddFirehoseRecord(record *{{recordType .Record}}) error {
	tags := []string{apptag.AppTag(), "deliverystream:{{.StreamName}}", "batch:true"}
	var rawRecord []byte
	rawRecord, err := json.Marshal(record)
	if err != nil {
		monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:false", "error:unmarshal", "type:client")...)
		return oops.Wrapf(err, "unable to marshal {{recordType .Record}} record ")
	}

	// Firehose PutRecordBatch API data size limit for a single record in a batch is 1000KB
	// Skip adding this record and continue
	if len(rawRecord) > recordSizeLimitBytes {
		monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:false", "error:sizelimit", "type:client")...)
		monitoring.AggregatedDatadog.Incr("firehose.send.record_size_limit_hit", tags...)
		// skiplint: +slogsensitivedata
		return oops.Errorf("Can not add data stream record in batch for stream {{.StreamName}} due to size limits. Max size is 1000KB. Size: %f Type %s", float64(len(rawRecord))/1000, "{{recordType .Record}}")
	}

	f.records = append(f.records, &firehose.Record{
		Data: rawRecord,
	})
	monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:true", "type:client")...)
	return nil
}

// WriteRecord puts a record of type {{recordType .Record}} to the {{.StreamName}} stream
func (w *{{pascalCase .StreamName}}StreamWriter) WriteRecord(ctx context.Context, record *{{recordType .Record}}) (err error) {
	{{- if not .EnableWriteOutsideECSEnv }}
	if !awsecs.IsRunningInECS() {
		return nil
	}
	{{ end }}
	tags := []string{apptag.AppTag(), "deliverystream:{{.StreamName}}", "batch:false"}
	defer func() {
		datadogutils.IncrInvocation(&err, "firehose.send.invocation", tags...)
	}()

	if err = ctx.Err(); err != nil {
		return oops.Wrapf(err, "Context cancelled before Firehose API call")
	}

	f := &FirehoseRecords{}
	err = f.ValidateAndAddFirehoseRecord(record)
	if err != nil {
		return oops.Wrapf(err, "Error adding firehose record for deliverystream:{{.StreamName}} error")
	}
	return w.WriteValidatedFirehoseRecords(ctx, f)
}

// WriteValidatedFirehoseRecords takes a list of validated firehose records and writes them
func (w *{{pascalCase .StreamName}}StreamWriter) WriteValidatedFirehoseRecords(ctx context.Context, f *FirehoseRecords) (err error) {
	{{- if not .EnableWriteOutsideECSEnv }}
	if !awsecs.IsRunningInECS() {
		return nil
	}
	{{ end }}
	tags := []string{apptag.AppTag(), "deliverystream:{{.StreamName}}"}
	defer func() {
		datadogutils.IncrInvocation(&err, "firehose.send.invocation", tags...)
	}()

	if err = ctx.Err(); err != nil {
		return oops.Wrapf(err, "Context cancelled before Firehose API call")
	}
	batchBytes := 0
	var recordBatches [][]*firehose.Record
	var recordBatch []*firehose.Record
	for _, record := range f.records {
		// When a batch hits 5MB is size or contains 500 records, add it recordBatches.
		// Firehose has strict limits on record and batch size so we create batches to stay under these limits.
		// Set recordBatch to an empty slice to start a new batch.
		if (batchBytes + len(record.Data)) >= batchSizeLimitBytes || len(recordBatch) == numberOfRecordsInBatchLimit {
			recordBatches = append(recordBatches, recordBatch)
			recordBatch = []*firehose.Record{}
			batchBytes = 0
		}

		recordBatch = append(recordBatch, record)
		batchBytes += len(record.Data)
	}

	// Append final batch if there are records in it
	if len(recordBatch) > 0 {
		recordBatches = append(recordBatches, recordBatch)
	}

	for _, recordBatch := range recordBatches {
		// Firehose PutRecordBatch API to send marshalled batch of {{recordType .Record}} to {{.StreamName}}
		_, err = w.firehoseClient.PutRecordBatchWithContext(ctx, &firehose.PutRecordBatchInput{
			DeliveryStreamName: pointer.New("{{.StreamName}}"),
			Records:            recordBatch,
		})
		if err != nil {
			if awserror.IsErrorAwsCanceled(err) {
				monitoring.AggregatedDatadog.Incr("firehose.write_batch", append(tags, "success:false", "error:contextcancelled" ,"type:server")...)
				return oops.Wrapf(err, "Context cancelled inside Firehose API call")
			}
			datadogutils.IncrResult(err, "firehose.send.record_count", tags...)
			monitoring.AggregatedDatadog.Incr("firehose.write_batch", append(tags, "success:false", "error:putrecord" ,"type:server")...)
			return oops.Wrapf(err, "error sending batch of raw {{recordType .Record}} to {{.StreamName}}")
		}
		datadogutils.IncrResult(err, "firehose.send.record_count", tags...)

		sendMetrics(recordBatch, tags)
	}
	return nil
}

// WriteRecordBatch sends a batch of records of type {{recordType .Record}} to the {{.StreamName}} stream
// AWS PutRecordBatch API has a limit of 500 records per batch and 5MB per batch.
// Paginate the API call based on these limits.
func (w *{{pascalCase .StreamName}}StreamWriter) WriteRecordBatch(ctx context.Context, records []*{{recordType .Record}}) (err error) {
	f := &FirehoseRecords{}
	for _, record := range records {
		err := f.ValidateAndAddFirehoseRecord(record)
		if err != nil {
			slog.Infow(ctx, fmt.Sprintf("Error adding firehose record for deliverystream:{{.StreamName}} error %s", err))
		}
	}
	return w.WriteValidatedFirehoseRecords(ctx, f)
}

// sendMetrics unpacks a batch of records and sends metrics to datadog
func sendMetrics(batch []*firehose.Record, tags []string) {
	batchBytes := 0
	for _, record := range batch {
		batchBytes += len(record.Data)
		monitoring.AggregatedDatadog.HistogramValue(float64(len(record.Data)), "firehose.send.record_size",tags...)
		monitoring.AggregatedDatadog.Count(int64(len(record.Data)), "firehose.send.total_size", tags...)
	}

	monitoring.AggregatedDatadog.Incr("firehose.send.batch_count", tags...)
	monitoring.AggregatedDatadog.HistogramValue(float64(len(batch)), "firehose.send.batch_length", tags...)
	monitoring.AggregatedDatadog.HistogramValue(float64(batchBytes), "firehose.send.batch_size_bytes", tags...)
	monitoring.AggregatedDatadog.Count(int64(len(batch)),"firehose.write_batch", append(tags, "success:true","type:server")...)
}
`

func GenerateFirehoseStreamWriters() error {
	err := validateRegistry()
	if err != nil {
		return oops.Wrapf(err, "Invalid registry")
	}

	templateFunctions := template.FuncMap{
		"pascalCase":        pascalCase,
		"recordType":        recordType,
		"removeUnderscores": removeUnderscores,
		"sortImports":       sortImports,
	}

	tmpl, err := template.New("firehose_writer_template").Funcs(templateFunctions).Parse(streamWriterFileTemplate)
	if err != nil {
		return oops.Wrapf(err, "unable to parse streamWriterTemplate string")
	}

	for _, entry := range Registry {
		directoryPath := filepath.Join(filepath.Join(filepathhelpers.BackendRoot, "go/src/samsaradev.io", entry.GeneratedWriterPath, fmt.Sprintf("%sstreamwriters", removeUnderscores(entry.StreamName))))
		if err := os.Mkdir(directoryPath, 0777); err != nil && !strings.Contains(string(err.Error()), "file exist") {
			return oops.Wrapf(err, "error creating directory at %s.", directoryPath)
		}

		filePath := filepath.Join(directoryPath, "writer.go")
		f, err := filehelpers.NewAtomicWriteFile(filePath)
		if err != nil {
			return oops.Wrapf(err, "error creating file at %s", filePath)
		}
		defer f.Close()

		codereviewFile, err := filehelpers.NewAtomicWriteFile(filepath.Join(directoryPath, "CODEREVIEW"))
		if err != nil {
			return oops.Wrapf(err, "error creating CODEREVIEW file at %s", filepath.Join(directoryPath, "CODEREVIEW"))
		}
		defer codereviewFile.Close()

		_, err = codereviewFile.Writer().Write([]byte(`{
  "owners": [
    "samsara-dev/data-platform"
  ],
	"description": "This directory/files are code-gen'd from 'infra/dataplatform/datastreamlake/genfirehosewriters.go' and changes to it require Data Platform approval."
}`))
		if err != nil {
			return oops.Wrapf(err, "error writing CODEREVIEW policy for registry entry %s", entry.StreamName)
		}

		if err := fileformatter.ExecuteTmplWithFormat(f.Writer(), entry, tmpl, fileformatter.Golang); err != nil {
			return oops.Wrapf(err, "unable to execute firehose_writer_template for %s stream", entry.StreamName)
		}

		if err := f.Save(); err != nil {
			return oops.Wrapf(err, "error saving writer file at %s", filePath)
		}
		if err := codereviewFile.Save(); err != nil {
			return oops.Wrapf(err, "error saving CODEREVIEW file at %s", filepath.Join(directoryPath, "CODEREVIEW"))
		}
	}

	return nil
}

func pascalCase(s string) string {
	pascalCase := ""
	for idx := 0; idx < len(s); idx++ {
		currChar := string(s[idx])
		if idx == 0 && idx != len(s)-1 {
			pascalCase += strings.ToUpper(currChar)
		} else if idx != 0 && currChar == "_" {
			pascalCase += strings.ToUpper(string(s[idx+1]))
			idx++
		} else {
			pascalCase += string(currChar)
		}
	}
	return pascalCase
}

func recordPackagePath(record interface{}) string {
	return reflect.TypeOf(record).PkgPath()
}

func recordType(record interface{}) string {
	recordPkg := filepath.Base(recordPackagePath(record))
	recordType := reflect.TypeOf(record).Name()
	return fmt.Sprintf("%s.%s", recordPkg, recordType)
}

func removeUnderscores(s string) string {
	return strings.Replace(s, "_", "", -1)
}

func sortImports(record any) []string {
	pkgPath := recordPackagePath(record)
	imports := []string{
		"samsaradev.io/infra/config",
		"samsaradev.io/helpers/appenv",
		"samsaradev.io/helpers/dogtag/apptag",
		"samsaradev.io/helpers/monitoring",
		"samsaradev.io/helpers/slog",
		"samsaradev.io/infra/fxregistry",
		"samsaradev.io/infra/monitoring/datadogutils",
		"samsaradev.io/infra/samsaraaws/awshelpers/awserror",
		"samsaradev.io/infra/samsaraaws/firehoseiface",
		"samsaradev.io/infra/samsaraaws/ni/awsecs",
		"samsaradev.io/libs/ni/pointer",
		pkgPath,
	}

	sort.Strings(imports)
	return imports
}

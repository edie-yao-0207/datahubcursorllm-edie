// Code generated by taskrunner gen/dataplatform/datastreamwriters; DO NOT EDIT.
package jackteststreamwriters

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/aws/aws-sdk-go/service/firehose"
	"github.com/samsarahq/go/oops"

	"samsaradev.io/helpers/dogtag/apptag"
	"samsaradev.io/helpers/monitoring"
	"samsaradev.io/helpers/slog"
	"samsaradev.io/infra/dataplatform/datastreamlake"
	"samsaradev.io/infra/fxregistry"
	"samsaradev.io/infra/monitoring/datadogutils"
	"samsaradev.io/infra/samsaraaws/awshelpers/awserror"
	"samsaradev.io/infra/samsaraaws/firehoseiface"
	"samsaradev.io/libs/ni/pointer"
)

const (
	bytesInMb                   = 1e6
	recordSizeLimitBytes        = 1 * bytesInMb // Firehose Record size limit is 1MB.
	batchSizeLimitBytes         = 4 * bytesInMb // Firehose PutRecord API Batch size limit is 4MB.
	numberOfRecordsInBatchLimit = 500           // Firehose PutRecord API number of records in a batch limit is 500.
)

type JackTestStreamWriter struct {
	firehoseClient firehoseiface.FirehoseAPI
}

func NewJackTestStreamWriter(firehoseAPI firehoseiface.FirehoseAPI) *JackTestStreamWriter {
	return &JackTestStreamWriter{
		firehoseClient: firehoseAPI,
	}
}

func init() {
	fxregistry.MustRegisterDefaultConstructor(NewJackTestStreamWriter)
}

type FirehoseRecords struct {
	records []*firehose.Record
}

// AddFirehoseRecordToBatch adds datastreamlake.TestRecord to a list of FirehoseRecords after validating the input
func (f *FirehoseRecords) ValidateAndAddFirehoseRecord(record *datastreamlake.TestRecord) error {
	tags := []string{apptag.AppTag(), "deliverystream:jack_test", "batch:true"}
	var rawRecord []byte
	rawRecord, err := json.Marshal(record)
	if err != nil {
		monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:false", "error:unmarshal", "type:client")...)
		return oops.Wrapf(err, "unable to marshal datastreamlake.TestRecord record ")
	}

	// Firehose PutRecordBatch API data size limit for a single record in a batch is 1000KB
	// Skip adding this record and continue
	if len(rawRecord) > recordSizeLimitBytes {
		monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:false", "error:sizelimit", "type:client")...)
		monitoring.AggregatedDatadog.Incr("firehose.send.record_size_limit_hit", tags...)
		// skiplint: +slogsensitivedata
		return oops.Errorf("Can not add data stream record in batch for stream jack_test due to size limits. Max size is 1000KB. Size: %f Type %s", float64(len(rawRecord))/1000, "datastreamlake.TestRecord")
	}

	f.records = append(f.records, &firehose.Record{
		Data: rawRecord,
	})
	monitoring.AggregatedDatadog.Incr("firehose.build_batch", append(tags, "success:true", "type:client")...)
	return nil
}

// WriteRecord puts a record of type datastreamlake.TestRecord to the jack_test stream
func (w *JackTestStreamWriter) WriteRecord(ctx context.Context, record *datastreamlake.TestRecord) (err error) {
	tags := []string{apptag.AppTag(), "deliverystream:jack_test", "batch:false"}
	defer func() {
		datadogutils.IncrInvocation(&err, "firehose.send.invocation", tags...)
	}()

	if err = ctx.Err(); err != nil {
		return oops.Wrapf(err, "Context cancelled before Firehose API call")
	}

	f := &FirehoseRecords{}
	err = f.ValidateAndAddFirehoseRecord(record)
	if err != nil {
		return oops.Wrapf(err, "Error adding firehose record for deliverystream:jack_test error")
	}
	return w.WriteValidatedFirehoseRecords(ctx, f)
}

// WriteValidatedFirehoseRecords takes a list of validated firehose records and writes them
func (w *JackTestStreamWriter) WriteValidatedFirehoseRecords(ctx context.Context, f *FirehoseRecords) (err error) {
	tags := []string{apptag.AppTag(), "deliverystream:jack_test"}
	defer func() {
		datadogutils.IncrInvocation(&err, "firehose.send.invocation", tags...)
	}()

	if err = ctx.Err(); err != nil {
		return oops.Wrapf(err, "Context cancelled before Firehose API call")
	}
	batchBytes := 0
	var recordBatches [][]*firehose.Record
	var recordBatch []*firehose.Record
	for _, record := range f.records {
		// When a batch hits 5MB is size or contains 500 records, add it recordBatches.
		// Firehose has strict limits on record and batch size so we create batches to stay under these limits.
		// Set recordBatch to an empty slice to start a new batch.
		if (batchBytes+len(record.Data)) >= batchSizeLimitBytes || len(recordBatch) == numberOfRecordsInBatchLimit {
			recordBatches = append(recordBatches, recordBatch)
			recordBatch = []*firehose.Record{}
			batchBytes = 0
		}

		recordBatch = append(recordBatch, record)
		batchBytes += len(record.Data)
	}

	// Append final batch if there are records in it
	if len(recordBatch) > 0 {
		recordBatches = append(recordBatches, recordBatch)
	}

	for _, recordBatch := range recordBatches {
		// Firehose PutRecordBatch API to send marshalled batch of datastreamlake.TestRecord to jack_test
		_, err = w.firehoseClient.PutRecordBatchWithContext(ctx, &firehose.PutRecordBatchInput{
			DeliveryStreamName: pointer.New("jack_test"),
			Records:            recordBatch,
		})
		if err != nil {
			if awserror.IsErrorAwsCanceled(err) {
				monitoring.AggregatedDatadog.Incr("firehose.write_batch", append(tags, "success:false", "error:contextcancelled", "type:server")...)
				return oops.Wrapf(err, "Context cancelled inside Firehose API call")
			}
			datadogutils.IncrResult(err, "firehose.send.record_count", tags...)
			monitoring.AggregatedDatadog.Incr("firehose.write_batch", append(tags, "success:false", "error:putrecord", "type:server")...)
			return oops.Wrapf(err, "error sending batch of raw datastreamlake.TestRecord to jack_test")
		}
		datadogutils.IncrResult(err, "firehose.send.record_count", tags...)

		sendMetrics(recordBatch, tags)
	}
	return nil
}

// WriteRecordBatch sends a batch of records of type datastreamlake.TestRecord to the jack_test stream
// AWS PutRecordBatch API has a limit of 500 records per batch and 5MB per batch.
// Paginate the API call based on these limits.
func (w *JackTestStreamWriter) WriteRecordBatch(ctx context.Context, records []*datastreamlake.TestRecord) (err error) {
	f := &FirehoseRecords{}
	for _, record := range records {
		err := f.ValidateAndAddFirehoseRecord(record)
		if err != nil {
			slog.Infow(ctx, fmt.Sprintf("Error adding firehose record for deliverystream:jack_test error %s", err))
		}
	}
	return w.WriteValidatedFirehoseRecords(ctx, f)
}

// sendMetrics unpacks a batch of records and sends metrics to datadog
func sendMetrics(batch []*firehose.Record, tags []string) {
	batchBytes := 0
	for _, record := range batch {
		batchBytes += len(record.Data)
		monitoring.AggregatedDatadog.HistogramValue(float64(len(record.Data)), "firehose.send.record_size", tags...)
		monitoring.AggregatedDatadog.Count(int64(len(record.Data)), "firehose.send.total_size", tags...)
	}

	monitoring.AggregatedDatadog.Incr("firehose.send.batch_count", tags...)
	monitoring.AggregatedDatadog.HistogramValue(float64(len(batch)), "firehose.send.batch_length", tags...)
	monitoring.AggregatedDatadog.HistogramValue(float64(batchBytes), "firehose.send.batch_size_bytes", tags...)
	monitoring.AggregatedDatadog.Count(int64(len(batch)), "firehose.write_batch", append(tags, "success:true", "type:server")...)
}

# Values to start up datahub after starting up the datahub-prerequisites chart with "prerequisites" release name
# Copy this chart and change configuration as needed.
datahub-gms:
  enabled: true
  replicaCount: 3
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-gms"
    tag: "v0.15.0.1"
  serviceAccount:
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::492164655156:role/datahub-eks-fargate-pods
  extraPodLabels:
    app.kubernetes.io/name: datahub-gms
  resources:
    # limits:
    #   memory: 2Gi
    requests:
      cpu: 6000m
      memory: 28Gi
  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  extraEnvs:
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "true"
    - name: AWS_REGION
      value: "us-west-2"
    - name: DATAHUB_TELEMETRY_ENABLED
      value: "false"
    - name: SEARCH_SERVICE_ENABLE_CACHE
      value: "true"
    - name: SEARCH_AUTHORIZATION_ENABLED
      value: "true"
  # - name: ELASTICSEARCH_USE_SSL
  #   value: "true"
  # - name: SKIP_ELASTICSEARCH_CHECK
  #   value: "true"
  # - name: LINEAGE_SEARCH_CACHE_ENABLED
  #   value: "true"
  # Optionaly specify service type for datahub-gms: LoadBalancer, ClusterIP or NodePort, by default: LoadBalancer
  service:
    type: LoadBalancer
    annotations:
      # old load balancer
      # service.beta.kubernetes.io/load-balancer-source-ranges: 52.38.67.177/32, 34.216.88.56/32, 35.167.55.1/32 # Dagster EKS nodes
      # service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "true"
      # service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "samsara-datahub-metadata"

      # new load balancer
      service.beta.kubernetes.io/aws-load-balancer-internal: "true"
      service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
      service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: "false"
      service.beta.kubernetes.io/aws-load-balancer-security-groups: "sg-0d4e998b9d1bb9324"

      # service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: "samsara-datahub-metadata"

      # service.beta.kubernetes.io/aws-load-balancer-scheme: "internal"
  # Optionally set a GMS specific SQL login (defaults to global login)
  # sql:
  #   datasource:
  #     username: "gms-login"
  #     password:
  #       secretRef: gms-secret
  #       secretKey: gms-password

datahub-frontend:
  enabled: true
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-frontend-prod"
    tag: "$COMMIT_SHA_SHORT_frontend"
    imagePullPolicy: "Always"
    # tag: "v0.11.0" # # defaults to .global.datahub.version
  serviceAccount:
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::492164655156:role/datahub-eks-fargate-pods
  extraPodLabels:
    app.kubernetes.io/name: datahub-frontend
  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  resources:
    limits:
      memory: 2800Mi
    requests:
      cpu: 400m
      memory: 2048Mi
  extraVolumes:
    - name: datahub-users
      secret:
        defaultMode: 0444
        secretName: datahub-users-secret
  extraVolumeMounts:
    - name: datahub-users
      mountPath: /datahub-frontend/conf/user.props
      subPath: user.props
  service:
    type: ClusterIP
  # Set up ingress to expose react front-end
  ingress:
    enabled: true
    defaultUserCredentials: {}
    className: alb
    extraLabels: {}
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/security-groups: "sg-01e73e63e43549506, sg-06361862cc973ab9b"
      # Ensure traffic is HTTPS
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: "443"
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:492164655156:certificate/13984a3e-7cdb-4cb7-83b6-35286db7e1ca
      # This list comes from OfficeCidrs in backend/go/src/samsaradev.io/libs/ni/infraconsts/cidrs.go
      alb.ingress.kubernetes.io/inbound-cidrs: 54.168.101.198/32, 3.111.155.138/32, 18.156.115.174/32, 18.132.101.7/32, 15.188.32.248/32, 54.225.84.247/32, 15.181.81.17/32, 15.181.200.153/32, 18.189.252.214/32, 50.18.205.11/32, 54.151.86.198/32, 44.231.79.181/32, 15.181.17.94/32
      alb.ingress.kubernetes.io/auth-type: cognito
      alb.ingress.kubernetes.io/auth-on-unauthenticated-request: authenticate
      alb.ingress.kubernetes.io/auth-idp-cognito: '{"userPoolARN":"arn:aws:cognito-idp:us-west-2:492164655156:userpool/us-west-2_roHc9TrhT","userPoolClientID":"59pon8m6vam95bb2ms2laefign","userPoolDomain":"samsara-datahub"}'
      alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
    hosts:
      - host: datahub.internal.samsara.com
        redirectPaths:
          - path: /*
            name: ssl-redirect
            port: use-annotation
        paths:
          - /*
  extraEnvs:
    - name: AUTH_JAAS_ENABLED
      value: "false"
    - name: AUTH_OIDC_ENABLED
      value: "true"
    - name: AUTH_OIDC_CLIENT_ID
      valueFrom:
        secretKeyRef:
          name: oidc-secrets
          key: oidc-client-id
    - name: AUTH_OIDC_CLIENT_SECRET
      valueFrom:
        secretKeyRef:
          name: oidc-secrets
          key: oidc-client-secret
    - name: AUTH_OIDC_DISCOVERY_URI
      value: https://samsararamp.samsara.com/.well-known/openid-configuration
    - name: AUTH_OIDC_BASE_URL
      value: https://datahub.internal.samsara.com
    - name: DATAHUB_AKKA_MAX_HEADER_VALUE_LENGTH
      value: 32k
    - name: DATAHUB_AKKA_MAX_HEADER_COUNT
      value: "128"

acryl-datahub-actions:
  enabled: false
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-acryl-datahub-actions"
    tag: "v0.15.0.1" # "v0.0.13" in official chart but aligned with our other internal tags here
  # mount the k8s secret as a volume in the container, each key name is mounted as a file on the mount path /etc/datahub/ingestion-secret-files
  # ingestionSecretFiles:
  #   name: ${K8S_SECRET_NAME}
  #   defaultMode: "0444"
  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi

datahub-mae-consumer:
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-mae-consumer"
    tag: "v0.15.0.1"
  serviceAccount:
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::492164655156:role/datahub-eks-fargate-pods
  extraPodLabels:
    app.kubernetes.io/name: datahub-mae-consumer
  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 300m
      memory: 1024Mi
  extraEnvs:
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "true"
    - name: AWS_REGION
      value: "us-west-2"
    - name: DATAHUB_TELEMETRY_ENABLED
      value: "false"
    - name: METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME
      value: "MetadataChangeLog_Versioned_v1"
    - name: METADATA_CHANGE_LOG_TIMESERIES_TOPIC_NAME
      value: "MetadataChangeLog_Timeseries_v1"

datahub-mce-consumer:
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-mce-consumer"
    tag: "v0.15.0.1"
    # tag: "v0.11.0" # defaults to .global.datahub.version
  serviceAccount:
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::492164655156:role/datahub-eks-fargate-pods
  extraPodLabels:
    app.kubernetes.io/name: datahub-mce-consumer
  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  readinessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30
    failureThreshold: 8
  resources:
    limits:
      memory: 1536Mi
    requests:
      cpu: 200m
      memory: 512Mi
  extraEnvs:
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "true"
    - name: AWS_REGION
      value: "us-west-2"
    - name: DATAHUB_TELEMETRY_ENABLED
      value: "false"
    - name: METADATA_CHANGE_EVENT_NAME
      value: "MetadataChangeEvent_v4"

datahub-ingestion-cron:
  enabled: false
  image:
    repository: acryldata/datahub-ingestion
    tag: "v0.15.0.1"

elasticsearchSetupJob:
  enabled: false
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-elasticsearch-setup"
    tag: "v0.15.0.1"
  extraEnvs:
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "true"
    - name: AWS_REGION
      value: "us-west-2"
    - name: ELASTICSEARCH_BUILD_INDICES_RETENTION_VALUE
      value: "7"
    - name: USE_AWS_ELASTICSEARCH
      value: "true"
  serviceAccount: "datahub-elasticsearch-setup"
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

kafkaSetupJob:
  enabled: true
  image:
    # repository: linkedin/datahub-kafka-setup
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-kafka-setup"
    tag: "v0.15.0.1"
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 300m
      memory: 768Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

mysqlSetupJob:
  enabled: true
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-mysql-setup"
    tag: "v0.15.0.1"
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 300m
      memory: 768Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "mysqlSetupJob-login"
  # password:
  #   secretRef: mysqlSetupJob-secret
  #   secretKey: mysqlSetupJob-password
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always

postgresqlSetupJob:
  enabled: false
  image:
    repository: acryldata/datahub-postgres-setup
    tag: "v0.15.0.1"
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  extraInitContainers: []
  podSecurityContext:
    fsGroup: 1000
  securityContext:
    runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-5"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Optionally set a set-up job specific login (defaults to global login)
  # username: "postgresqlSetupJob-login"
  # password:
  #   secretRef: postgresqlSetupJob-secret
  #   secretKey: postgresqlSetupJob-password
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  # Optionally set a specific database using extraEnvs
  # extraEnvs: []
  #   - name: "DATAHUB_DB_NAME"
  #     value: "dh"

## No code data migration
datahubUpgrade:
  enabled: true
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-upgrade"
    tag: "v0.15.0.1"
  batchSize: 1000
  batchDelayMs: 100
  noCodeDataMigration:
    sqlDbType: "MYSQL"
    # sqlDbType: "POSTGRES"
  podSecurityContext:
    {}
    # fsGroup: 1000
  securityContext:
    {}
    # runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "-2"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  cleanupJob:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    # Add the concurrency Policy flexibility via values
    concurrencyPolicy: Allow
    # Add extra sidecar containers to job pod
    extraSidecars:
      []
      # - name: my-image-name
      #   image: my-image
      #   imagePullPolicy: Always
  restoreIndices:
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 300m
        memory: 256Mi
    # Add the concurrency Policy flexibility via values
    concurrencyPolicy: Allow
    # Add extra sidecar containers to job pod
    extraSidecars:
      []
      # - name: my-image-name
      #   image: my-image
      #   imagePullPolicy: Always
    # Optionally set a specific PostgreSQL database name using extraEnvs
    # extraEnvs:
    #   - name: "DATAHUB_DB_NAME"
    #    value: "dh"
  extraInitContainers: []

## Runs system update processes
## Includes: Elasticsearch Indices Creation/Reindex (See global.elasticsearch.index for additional configuration)
datahubSystemUpdate:
  image:
    repository: "492164655156.dkr.ecr.us-west-2.amazonaws.com/datahub-upgrade"
    tag: "v0.15.0.1"
  podSecurityContext:
    {}
    # fsGroup: 1000
  securityContext:
    {}
    # runAsUser: 1000
  annotations:
    # This is what defines this resource as a hook. Without this line, the
    # job is considered part of the release.
    helm.sh/hook: pre-install,pre-upgrade
    helm.sh/hook-weight: "-4"
    helm.sh/hook-delete-policy: before-hook-creation
  podAnnotations: {}
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 300m
      memory: 256Mi
  # Add extra sidecar containers to job pod
  extraSidecars:
    []
    # - name: my-image-name
    #   image: my-image
    #   imagePullPolicy: Always
  extraInitContainers: []
  extraEnvs:
    - name: OPENSEARCH_USE_AWS_IAM_AUTH
      value: "false"
    - name: ELASTICSEARCH_BUILD_INDICES_RETENTION_VALUE
      value: "7"
  # - name: AWS_REGION
  #   value: "us-west-2"

global:
  strict_mode: true
  graph_service_impl: elasticsearch
  datahub_analytics_enabled: true
  datahub_standalone_consumers_enabled: true

  elasticsearch:
    #staging
    # host: "elasticsearch-master"
    # port: "9200"
    skipcheck: "true"
    # insecure: "false"
    # useSSL: "false"

    # production
    host: vpc-datahub-i5qct4qnqglgwc443cqynogglm.us-west-2.es.amazonaws.com
    port: "443"
    useSSL: "true"
    region: "us-west-2"

    # auth:
    #   username: datahub
    #   password:
    #       secretRef: elasticsearch-secrets
    #       secretKey: elasticsearch-password
    extraEnvs:
      - name: OPENSEARCH_USE_AWS_IAM_AUTH
        value: "true"
      - name: AWS_REGION
        value: "us-west-2"
    # If you want to specify index prefixes use indexPrefix
    # indexPrefix: "dh"

    ## The following section controls when and how reindexing of elasticsearch indices are performed
    index:
      ## Enable reindexing when mappings change based on the data model annotations
      enableMappingsReindex: true

      ## Enable reindexing when static index settings change.
      ## Dynamic settings which do not require reindexing are not affected
      ## Primarily this should be enabled when re-sharding is necessary for scaling/performance.
      enableSettingsReindex: true

      ## Index settings can be overridden for entity indices or other indices on an index by index basis
      ## Some index settings, such as # of shards, requires reindexing while others, i.e. replicas, do not
      ## Non-Entity indices do not require the prefix
      # settingsOverrides: '{"graph_service_v1":{"number_of_shards":"5"},"system_metadata_service_v1":{"number_of_shards":"5"}}'
      ## Entity indices do not require the prefix or suffix
      # entitySettingsOverrides: '{"dataset":{"number_of_shards":"10"}}'

      ## The amount of delay between indexing a document and having it returned in queries
      ## Increasing this value can improve performance when ingesting large amounts of data
      # refreshIntervalSeconds: 1

      ## The following options control settings for datahub-upgrade job when creating or reindexing indices
      upgrade:
        ## When reindexing is required, this option will clone the existing index as a backup
        ## The clone indices are not currently managed.
        cloneIndices: true

        ## Typically when reindexing the document counts between the original and destination indices should match.
        ## In some cases reindexing might not be able to proceed due to incompatibilities between a document in the
        ## orignal index and the new index's mappings. This document could be dropped and re-ingested or restored from
        ## the SQL database.
        ##
        ## This setting allows continuing if and only if the cloneIndices setting is also enabled which
        ## ensures a complete backup of the original index is preserved.
        allowDocCountMismatch: false

    ## Search related configuration
    search:
      ## Maximum terms in aggregations
      maxTermBucketSize: 20

      ## Configuration around exact matching for search
      exactMatch:
        ## if false will only apply weights, if true will exclude non-exact
        exclusive: false
        ## include prefix exact matches
        withPrefix: true
        ## boost multiplier when exact with case
        exactFactor: 2.0
        ## boost multiplier when exact prefix
        prefixFactor: 1.6
        ## stacked boost multiplier when case mismatch
        caseSensitivityFactor: 0.7
        ## enable exact match on structured search
        enableStructured: true

      ## Configuration for graph service dao
      graph:
        ## graph dao timeout seconds
        timeoutSeconds: 50
        ## graph dao batch size
        batchSize: 1000
        ## graph dao max result size
        maxResult: 10000

      custom:
        enabled: true
        # See documentation: https://datahubproject.io/docs/how/search/#customizing-search
        config:
          # Notes:
          #
          # First match wins
          #
          # queryRegex = Java regex syntax
          #
          # functionScores - See the following for function score syntax
          # https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-function-score-query.html

          queryConfigurations:
            # Select *
            - queryRegex: .*

              simpleQuery: true
              prefixMatchQuery: true
              exactMatchQuery: true

              boolQuery:
                must_not:
                  term:
                    deprecated:
                      value: true

              functionScore:
                functions:
                  - filter:
                      terms:
                        domains.keyword:
                          - urn:li:domain:datamodel
                    weight: 2.0

                  - filter:
                      terms:
                        domains.keyword:
                          - urn:li:domain:kinesis_stats
                    weight: 3.0

                  - filter:
                      terms:
                        domains.keyword:
                          - urn:li:domain:metrics_repo
                    weight: 3.0

                  - filter:
                      term:
                        customProperties.storage_location: s3://samsara-datamodel-warehouse/datamodel_core.db/dim_organizations
                    weight: 2
                  - filter:
                      term:
                        customProperties.storage_location.keyword: s3://samsara-datamodel-warehouse/datamodel_core.db/dim_organizations
                    weight: 2
                  - filter:
                      term:
                        name.keyword: datamodel_core.dim_organizations
                    weight: 2
                  - filter:
                      term:
                        customProperties.storage_location: s3://samsara-datamodel-warehouse/datamodel_core.db/dim_devices
                    weight: 2
                  - filter:
                      term:
                        customProperties.storage_location.keyword: s3://samsara-datamodel-warehouse/datamodel_core.db/dim_devices
                    weight: 2
                  - filter:
                      term:
                        name.keyword: datamodel_core.dim_devices
                    weight: 2
                  - filter:
                      terms:
                        domains.keyword:
                          - urn:li:domain:notebooks
                    weight: 0.2

                  - filter:
                      terms:
                        glossaryTerms.keyword:
                          # Certified tables
                          - urn:li:glossaryTerm:1dd7fa08-22a3-4e10-95d3-b671b0100439
                    weight: 5.0
                score_mode: multiply
                boost_mode: multiply

  kafka:
    bootstrap:
      server: "prerequisites-kafka:9092"
    zookeeper:
      server: "prerequisites-zookeeper:2181"
    # This section defines the names for the kafka topics that DataHub depends on, at a global level. Do not override this config
    # at a sub-chart level.
    topics:
      metadata_change_event_name: "MetadataChangeEvent_v4"
      failed_metadata_change_event_name: "FailedMetadataChangeEvent_v4"
      metadata_audit_event_name: "MetadataAuditEvent_v4"
      datahub_usage_event_name: "DataHubUsageEvent_v1"
      metadata_change_proposal_topic_name: "MetadataChangeProposal_v1"
      failed_metadata_change_proposal_topic_name: "FailedMetadataChangeProposal_v1"
      metadata_change_log_versioned_topic_name: "MetadataChangeLog_Versioned_v1"
      metadata_change_log_timeseries_topic_name: "MetadataChangeLog_Timeseries_v1"
      platform_event_topic_name: "PlatformEvent_v1"
      datahub_upgrade_history_topic_name: "DataHubUpgradeHistory_v1"
    consumer_groups:
      datahub_upgrade_history_kafka_consumer_group_id: {}
      #   gms: "<<release-name>>-duhe-consumer-job-client-gms"
      #   mae-consumer: "<<release-name>>-duhe-consumer-job-client-mcl"
      #   mce-consumer: "<<release-name>>-duhe-consumer-job-client-mcp"
      datahub_actions_ingestion_executor_consumer_group_id: "ingestion_executor"
      datahub_actions_slack_consumer_group_id: "datahub_slack_action"
      datahub_actions_teams_consumer_group_id: "datahub_teams_action"
      datahub_usage_event_kafka_consumer_group_id: "datahub-usage-event-consumer-job-client"
      metadata_change_log_kafka_consumer_group_id: "generic-mae-consumer-job-client"
      platform_event_kafka_consumer_group_id: "generic-platform-event-job-client"
      metadata_change_event_kafka_consumer_group_id: "mce-consumer-job-client"
      metadata_change_proposal_kafka_consumer_group_id: "generic-mce-consumer-job-client"
    metadataChangeLog:
      hooks:
        siblings:
          enabled: true
          consumerGroupSuffix: ""
        updateIndices:
          enabled: true
          consumerGroupSuffix: ""
        ingestionScheduler:
          enabled: true
          consumerGroupSuffix: ""
        incidents:
          enabled: true
          consumerGroupSuffix: ""
        entityChangeEvents:
          enabled: true
          consumerGroupSuffix: ""
        forms:
          enabled: true
          consumerGroupSuffix: ""
    maxMessageBytes: "5242880" # 5MB
    producer:
      compressionType: none
      maxRequestSize: "5242880" # 5MB
    consumer:
      maxPartitionFetchBytes: "5242880" # 5MB
      stopContainerOnDeserializationError: true
    ## For AWS MSK set this to a number larger than 1
    # partitions: 3
    # replicationFactor: 3
    schemaregistry:
      # GMS Implementation - `url` configured based on component context
      type: INTERNAL
      # Confluent Kafka Implementation
      # type: KAFKA
      # url: "http://prerequisites-cp-schema-registry:8081"

      # Glue Implementation - `url` not applicable
      # type: AWS_GLUE
      # glue:
      #   region: us-east-1
      #   registry: datahub

  neo4j:
    host: "prerequisites-neo4j:7474"
    uri: "bolt://prerequisites-neo4j"
    username: "neo4j"
    password:
      secretRef: neo4j-secrets
      secretKey: neo4j-password
    # --------------OR----------------
    # value: password

  sql:
    datasource:
      host: "datahub-prod-mysql.cluster-cv4bckzywclc.us-west-2.rds.amazonaws.com:3306"
      hostForMysqlClient: "datahub-prod-mysql.cluster-cv4bckzywclc.us-west-2.rds.amazonaws.com"
      port: "3306"
      url: "jdbc:mysql://datahub-prod-mysql.cluster-cv4bckzywclc.us-west-2.rds.amazonaws.com:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8"
      driver: "com.mysql.jdbc.Driver"
      username: "datahub"
      password:
        secretRef: mysql-secrets-v2
        secretKey: mysql-root-password
      # host: "prerequisites-mysql:3306"
      # hostForMysqlClient: "prerequisites-mysql"
      # port: "3306"
      # url: "jdbc:mysql://prerequisites-mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      # driver: "com.mysql.cj.jdbc.Driver"
      # username: "root"
      # password:
      #   secretRef: mysql-secrets
      #   secretKey: mysql-root-password

      # --------------OR----------------
      # value: password

      ## Use below for usage of PostgreSQL instead of MySQL
      # host: "prerequisites-postgresql:5432"
      # hostForpostgresqlClient: "prerequisites-postgresql"
      # port: "5432"
      # url: "jdbc:postgresql://prerequisites-postgresql:5432/datahub"
      # driver: "org.postgresql.Driver"
      # username: "postgres"
      # password:
      #   secretRef: postgresql-secrets
      #   secretKey: postgres-password
      # --------------OR----------------
      #   value: password

      # If you want to use specific PostgreSQL database use extraEnvs
      # extraEnvs:
      #  - name: "DATAHUB_DB_NAME"
      #    value: "dh"

      ## Use below for usage of Google Cloud SQL MySQL instance instead of the self hosted MySQL,
      ## make sure you have deployed gcloud-sqlproxy as prerequisite
      # host: "prerequisites-gcloud-sqlproxy:3306"
      # hostForMysqlClient: "prerequisites-gcloud-sqlproxy"
      # port: "3306"
      # url: "jdbc:mysql://prerequisites-gcloud-sqlproxy:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8&enabledTLSProtocols=TLSv1.2"
      # driver: "com.mysql.cj.jdbc.Driver"
      # username: "root"
      # password:
      #   secretRef: mysql-secrets
      #   secretKey: mysql-root-password
      # --------------OR----------------
      #   value: password

  datahub:
    version: v0.15.0.1
    gms:
      port: "8080"
      nodePort: "30001"

    frontend:
      validateSignUpEmail: false

    monitoring:
      enablePrometheus: false

    # cache:
    #   search:
    #     ## Enable general search caching
    #     enabled: true
    #     ## Configuration for the primary cahe
    #     primary:
    #       ttlSeconds: 600
    #       maxSize: 10000
    #     ## Configuration for homepage cache
    #     homepage:
    #       entityCounts:
    #         ttlSeconds: 600
    #     ## Lineage specific caching options
    #     lineage:
    #       ## Enables in-memory cache for searchAcrossLineage query
    #       enabled: false
    #       ttlSeconds: 86400
    #       lightningThreshold: 300

    mae_consumer:
      port: "9091"
      nodePort: "30002"

    appVersion: "1.0"
    systemUpdate:
      ## The following options control settings for datahub-upgrade job which will
      ## managed ES indices and other update related work
      enabled: false # this should only be true when performing a version upgrade of DataHub (e.g. 0.13.0 -> 0.15.0.1)

    encryptionKey:
      secretRef: "datahub-encryption-secrets"
      secretKey: "encryption_key_secret"
      # Set to false if you'd like to provide your own secret.
      provisionSecret:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    encryptionKey: <encryption key value>

    managed_ingestion:
      enabled: true
      defaultCliVersion: "0.15.0.1"

    metadata_service_authentication:
      enabled: true
      systemClientId: "__datahub_system"
      systemClientSecret:
        secretRef: "datahub-auth-secrets"
        secretKey: "system_client_secret"
      tokenService:
        signingKey:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_signing_key"
        salt:
          secretRef: "datahub-auth-secrets"
          secretKey: "token_service_salt"
      # Set to false if you'd like to provide your own auth secrets
      provisionSecrets:
        enabled: true
        autoGenerate: true
        annotations: {}
      # Only specify if autoGenerate set to false
      #  secretValues:
      #    secret: <secret value>
      #    signingKey: <signing key value>
      #    salt: <salt value>

    ## Enables always emitting a MCL even when no changes are detected. Used for Time Based Lineage when no changes occur.
    alwaysEmitChangeLog: false

    ## Enables diff mode for graph writes, uses a different code path that produces a diff from previous to next to write relationships instead of wholesale deleting edges and reading
    enableGraphDiffMode: true

    ## Values specific to the unified search and browse feature.
    search_and_browse:
      show_search_v2: true # If on, show the new search filters experience as of v0.10.5
      show_browse_v2: true # If on, show the new browse experience as of v0.10.5
      backfill_browse_v2: true # If on, run the backfill upgrade job that generates default browse paths for relevant entities

#  hostAliases:
#    - ip: "192.168.0.104"
#      hostnames:
#        - "broker"
#        - "mysql"
#        - "postgresql"
#        - "elasticsearch"
#        - "neo4j"

## Add below to enable SSL for kafka
#  credentialsAndCertsSecrets:
#    name: datahub-certs
#    path: /mnt/datahub/certs
#    secureEnv:
#      ssl.key.password: datahub.linkedin.com.KeyPass
#      ssl.keystore.password: datahub.linkedin.com.KeyStorePass
#      ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#      kafkastore.ssl.truststore.password: datahub.linkedin.com.TrustStorePass
#
#  springKafkaConfigurationOverrides:
#    ssl.keystore.location: /mnt/datahub/certs/datahub.linkedin.com.keystore.jks
#    ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    kafkastore.ssl.truststore.location: /mnt/datahub/certs/datahub.linkedin.com.truststore.jks
#    security.protocol: SSL
#    kafkastore.security.protocol: SSL
#    ssl.keystore.type: JKS
#    ssl.truststore.type: JKS
#    ssl.protocol: TLS
#    ssl.endpoint.identification.algorithm:
